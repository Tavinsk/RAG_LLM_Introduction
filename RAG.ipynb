{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importando as bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader # Permite carregar arquivos de texto\n",
    "from langchain_text_splitters import CharacterTextSplitter # Permite dividir o texto em partes menores\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore # Permite armazenar vetores no Pinecone\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain import hub\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Carregar as vari√°veis de ambientes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **INSTANCIAR O BANCO DE DADOS DO PINECONE**\n",
    "\n",
    "OBS: Devemos criar o index no pinecone cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"rag-demo\"\n",
    "embeddings = CohereEmbeddings(\n",
    "    model=\"embed-english-v3.0\",\n",
    "    cohere_api_key=COHERE_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Criando a m√©moria**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tavin\\AppData\\Local\\Temp\\ipykernel_9588\\515166408.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CARREGAMENTO/INGEST√ÉO DO DOCUMENTO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando os documentos...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\tavin\\AppData\\Local\\Temp\\ipykernel_9588\\3164062685.py:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  PATH_FILE = \"data\\mediumblog1.txt\"\n"
     ]
    }
   ],
   "source": [
    "print(\"üî•Carregando os documentos...\\n\\n\")\n",
    "PATH_FILE = \"data\\mediumblog1.txt\"\n",
    "loader = TextLoader(PATH_FILE)\n",
    "document = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **FATIMANETO/SPLITTING DOS DOCUMENTOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Spplitting the document into smaller chunks...\")\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "docs = text_splitter.split_documents(document)\n",
    "print(f\"Total chunks: {len(docs)}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Instanciar o banco de dados**\n",
    "\n",
    "Ap√≥s a execu√ß√£o deste comando, os dados estar√£o disponiveis no banco de dados cloud no Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_from_docs = PineconeVectorStore.from_documents(\n",
    "    docs,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Realizando consultas no banco de dados vetorial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = PineconeVectorStore(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='455ba684-4813-467e-9d39-8d00a4c9a12f', metadata={'source': 'data\\\\mediumblog1.txt'}, page_content='Title: Vector Database: What is it and Why You Should Know It?\\n\\nAuthor: Ejiro Onose\\nDate: December 22, 2023\\n\\nIf 2021 was the year of graph databases, 2023 is the year of vector databases √¢‚Ç¨‚Äù Chip Huen.\\n\\nGenerative AI and Large Language Models (LLMs) have become popular, and a vector database is one of the best tools to handle LLM data. Vector databases provide the ideal infrastructure for managing the complex, high-dimensional data that LLMs produce and rely upon.\\n\\nIn this article, I√¢‚Ç¨‚Ñ¢ll explain what vector databases are, how they work, and introduce some top vector database tools.\\n\\n What is a Vector?\\nIn machine learning (ML), a vector is a collection of numerical values that represents the features of multi-dimensional objects, such as words or images. For example, a vector representing an image might contain values related to pixel intensities and color channels.')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Vector Store?\"\n",
    "vectorstore.similarity_search(query, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CRIAR LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model = \"Gemma2-9b-It\", # MODELO DE LLM UTILIZADO\n",
    "    groq_api_key = GROQ_API_KEY, # CHAVE DE API DO GROQ\n",
    "    temperature = 0.1, # TEMPERATURA DO LLM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Carregar um prompt \"padr√£o\" via langchain hub** e criar o retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tavin\\RAG_LLM_Introduction\\.venv\\Lib\\site-packages\\langsmith\\client.py:278: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "combine_docs_chain   = create_stuff_documents_chain(\n",
    "    llm, retrieval_qa_chat_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Criar a chain com base no prompt e retriver**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = create_retrieval_chain(\n",
    "    vectorstore.as_retriever(),\n",
    "    combine_docs_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A vector store is a specialized database designed to efficiently store and retrieve high-dimensional vectors. \n",
      "\n",
      "These vectors represent data points like words, images, or audio, capturing their semantic meaning or characteristics. \n",
      "\n",
      "Vector stores excel at similarity searches, allowing you to find data points that are closest to a given query vector. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = qa.invoke({\"input\": \"What is vector store in 3 setences?\"})\n",
    "print(res['answer']) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
